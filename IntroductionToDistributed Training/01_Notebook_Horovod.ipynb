{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling to Multiple GPUs with Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Horovod](https://github.com/horovod/horovod) is a distributed deep learning training framework. It is available for TensorFlow, Keras, PyTorch, and Apache MXNet. In this lab you will learn about what Horovod is and how to use it, by distributing across multiple GPUs the training of the classification model we started with in Exercise 3 of Lab 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Outline\n",
    "\n",
    "The progression of this lab is as follows:\n",
    "\n",
    "- A high level introduction to Horovod, including its ties to the parallel computing protocol MPI, and the additional details that must be taken into account when using a parallel computing framework like Horovod.\n",
    "- An overview and initial run of the existing code base that you will be refactoring with Horovod, which is a classification model using Keras and the Fashion-MNIST dataset, currently built to run on a single GPU.\n",
    "- A multi-step refactor of the existing code base so that it uses Horovod to run distributed across this environment's available GPUs, introducing Horovod concepts and techniques throughout.\n",
    "- A final run of the refactored and distributed code base, with discussion of its speed up.\n",
    "\n",
    "This lab draws heavily on content provided in the [Horovod tutorials](https://github.com/horovod/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the time you complete this lab you will be able to:\n",
    "\n",
    "- Discuss what Horovod is, how it works, and why it is an effective tool for distributed training.\n",
    "- Use Horovod to refactor or build deep learning models that train distributed across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Horovod\n",
    "\n",
    "[Horovod](https://github.com/horovod/horovod) is an open source tool originally [developed by Uber](https://eng.uber.com/horovod/) to support their need for faster deep learning model training across their many engineering teams. It is part of a growing ecosystem of approaches to distributed training, including for example [Distributed TensorFlow](https://www.tensorflow.org/deploy/distributed). Uber decided to develop a solution that utilized [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) for distributed process communication, and the [NVIDIA Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl) for its highly optimized implementation of reductions across distributed processes and nodes. The resulting Horovod package delivers on its promise to scale deep learning model training across multiple GPUs and multiple nodes, with only minor code modification and intuitive debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its inception in 2017 Horovod has matured significantly, extending its support from just TensorFlow to Keras, PyTorch, and Apache MXNet. Horovod is extensively tested and has been used on some of the largest DL training runs done to date, for example, supporting **exascale** deep learning on the [Summit system, scaling to over **27,000 V100 GPUs**](https://arxiv.org/pdf/1810.01993.pdf):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![horovod scaling](./images/horovod_scaling.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Horovod now so that we can query it later on. The convention is to import it as `hvd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import horovod.keras as hvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horovod's MPI Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod's connection to MPI runs deep, and for programmers familiar with MPI programming, much of what you program to distribute model training with Horovod will feel very familiar. For those unfamiliar with MPI programming, a brief discussion of some of the conventions and considerations required when distributing processes with Horovod, or MPI, is worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod, as with MPI, strictly follows the [Single-Program Multiple-Data (SPMD) paradigm](https://en.wikipedia.org/wiki/SPMD) where we implement the instruction flow of multiple processes in the same file/program. Because multiple processes are executing code in parallel, we have to take care about [race conditions](https://en.wikipedia.org/wiki/Race_condition) and also the synchronization of participating processes.\n",
    "\n",
    "Horovod assigns a unique numerical ID or **rank** (an MPI concept) to each process executing the program. This rank can be accessed programmatically. As you will see below when writing Horovod code, by identifying a process's rank programmatically in the code we can take steps such as:\n",
    "\n",
    "- Pin that process to its own exclusive GPU.\n",
    "- Utilize a single rank for broadcasting values that need to be used uniformly by all ranks.\n",
    "- Utilize a single rank for collecting and/or reducing values produced by all ranks.\n",
    "- Utilize a single rank for logging or writing to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work through this course, keep these concepts in mind and especially that Horovod will be sending your single program to be executed in parallel by multiple processes. Keeping this in mind will support your intuition and understanding about why we do what we do with Horovod, even though you will only be making edits to a single program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Existing Model Files\n",
    "\n",
    "On the left hand side of this lab environment, you will see a file directory with this notebook, a couple of Python files and a `solutions` directory.\n",
    "\n",
    "The file `fashion_mnist.py` contains the Keras model that does not have any Horovod code while `solutions/fashion_mnist_solution.py` has all the Horovod features added. In this tutorial, we will guide you to transform `fashion_mnist.py` into `solutions/fashion_mnist_solution.py` step-by-step. As you work through exercises to complete this task, you can, if needed compare your code with the `solutions/fashion_mnist_after_step_N.py` files that correspond to the step you are at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: train the model\n",
    "\n",
    "Before we go into modifications required to scale our WideResNet model, please make sure you can train the single GPU version of the model. This is the same as where we left off at the end of Lab 1, except that now we are using the full dataset. We'll just run a few epochs with a relatively large batch size. This will take a bit longer than before, so feel free to read ahead while this is training. Take note of how long the training took when it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2021-03-25 14:45:47.463556: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 14:45:51.264519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 14:45:51.265812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1b.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 14:45:51.384462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 14:45:51.385720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1c.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 14:45:51.498856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 14:45:51.514668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1d.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 14:45:51.633211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 14:45:51.634462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 14:45:51.634533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2021-03-25 14:45:52.887635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 14:45:52.887691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 \n",
      "2021-03-25 14:45:52.887703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y Y Y \n",
      "2021-03-25 14:45:52.887719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N Y Y \n",
      "2021-03-25 14:45:52.887733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   Y Y N Y \n",
      "2021-03-25 14:45:52.887747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   Y Y Y N \n",
      "2021-03-25 14:45:52.887941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14971 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n",
      "2021-03-25 14:45:52.888226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14971 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n",
      "2021-03-25 14:45:52.888428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14971 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n",
      "2021-03-25 14:45:52.888611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14971 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n",
      "Epoch 1/5\n",
      "118/118 [==============================] - 54s 456ms/step - loss: 1.9673 - acc: 0.5805 - val_loss: 0.8278 - val_acc: 0.7069\n",
      "Images/sec: 1158.16\n",
      "Cumulative training time after epoch 1: 53.82\n",
      "Epoch 2/5\n",
      "118/118 [==============================] - 37s 315ms/step - loss: 0.7011 - acc: 0.7519 - val_loss: 0.6921 - val_acc: 0.7579\n",
      "Images/sec: 1612.39\n",
      "Cumulative training time after epoch 2: 91.03\n",
      "Epoch 3/5\n",
      "118/118 [==============================] - 37s 315ms/step - loss: 0.6232 - acc: 0.7781 - val_loss: 0.7383 - val_acc: 0.7556\n",
      "Images/sec: 1612.23\n",
      "Cumulative training time after epoch 3: 128.25\n",
      "Epoch 4/5\n",
      "118/118 [==============================] - 37s 316ms/step - loss: 0.5589 - acc: 0.8035 - val_loss: 0.6483 - val_acc: 0.7678\n",
      "Images/sec: 1611.52\n",
      "Cumulative training time after epoch 4: 165.48\n",
      "Epoch 5/5\n",
      "118/118 [==============================] - 37s 316ms/step - loss: 0.5185 - acc: 0.8188 - val_loss: 0.7915 - val_acc: 0.7423\n",
      "Images/sec: 1610.79\n",
      "Cumulative training time after epoch 5: 202.73\n",
      "Cumulative training time: 202.73\n",
      "Test loss: 0.7923941128730774\n",
      "Test accuracy: 0.7416\n"
     ]
    }
   ],
   "source": [
    "!python fashion_mnist.py --epochs 5 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the training script\n",
    "\n",
    "We are going to start making modifications to the training script. Before we do, let's make a copy of it on disk -- that way, if you make a mistake and want to back up to the beginning, you have a reference copy to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp fashion_mnist.py fashion_mnist_original.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click `fashion_mnist.py` in the left pane to open it in the editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize Horovod and select the GPU to run on\n",
    "\n",
    "Naturally we need to start by importing Horovod. \n",
    "\n",
    "**Exercise**: Add `import horovod.keras as hvd` to the training script and initialize Horovod before the argument parsing:\n",
    "\n",
    "```python\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "```\n",
    "\n",
    "(look for the `TODO: Step 1` lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Horovod, which can run multiple processes across multiple GPUs, you typically use a single GPU per training process. Part of what makes Horovod simple to use is that it utilizes MPI, and as such, uses much of the MPI nomenclature. The concept of a **rank** in MPI is of a unique process ID. In this lab we will be using the term \"rank\" extensively. If you would like to know more about MPI concepts that are utilized heavily in Horovod, please refer to [the Horovod documentation](https://github.com/horovod/horovod/blob/master/docs/concepts.rst).\n",
    "\n",
    "Schematically, let's look at how MPI can run multiple GPU processes across multiple nodes. Note how each process, or rank, is pinned to a specific GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/16640218/53518255-7d5fc300-3a85-11e9-8bf3-5d0e8913c14f.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method we do not have to deal with placing specific data on specific GPUs. Instead, you just specify which GPU you would like to use in the beginning of your script. \n",
    "\n",
    "Before we get there, let's refresh ourselves on how to work with multiple GPUs on a node. On the NVIDIA platform, CUDA, if we have N GPUs they are uniquely numbered from 0 to N-1. In this lab we won't worry about how the numbering is selected or whether the order matters. If we want to pin our process to use a specific GPU, say GPU 1, we can use the following TensorFlow/Keras code:\n",
    "\n",
    "```python\n",
    "# Pin to GPU 1\n",
    "gpu_id = 1\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(gpu_id)\n",
    "K.set_session(tf.Session(config=config))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `visible_device_list` is a comma-separated string of all of the GPUs that this TensorFlow process is allowed to know about, from which it will select one to use by default. If you want to use multiple GPUs from this list, you can do so by manually controlling which \"device\" to use (see TensorFlow's [documentation](https://www.tensorflow.org/guide/gpu) for more information). Typically one would do so when manual control over the distribution of data is needed, and a common use case is model parallelism (which Horovod [does not natively support](https://github.com/horovod/horovod/issues/96)). In this lab we will only be using one GPU per rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test your understanding of how this works. First, let's identify how many GPUs are on our node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 25 14:53:13 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   31C    P0    47W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   32C    P0    48W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   32C    P0    53W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   30C    P0    51W / 300W |      0MiB / 16160MiB |      4%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"!\" prefix means that we execute the above in the terminal; now let's do this in actual terminal. Open a new launcher (File > New Launcher in the menu bar), select the \"Terminal\" option, execute the `nvidia-smi` command there, and verify it provides the same output. Notice that there is a \"GPU-Util\" column, which measures the GPU's utilization. It tells you what fraction of the last second the GPU was in use. We can thus easily monitor GPU activity by regularly checking this output. One way to do that is using the Linux utility [watch](https://en.wikipedia.org/wiki/Watch_(Unix)): `watch -n 5 nvidia-smi` will set up a loop that refreshes the `nvidia-smi` output every 5 seconds. Make sure you run that in the separate terminal window, not here in the notebook, because the notebook can only run one process at a time. You can type Ctrl+C in the terminal to end the loop later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: set up `nvidia-smi` to regularly monitor the GPU activity in a terminal as above, and then here in the notebook start a training process. Then switch back to the terminal and watch the GPU activity. Can you verify that only one GPU is used? Does it match the GPU ID you asked for in the training script? Also, keep an eye out for other utilization metrics like power consumption and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 14:57:01.938876: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 14:57:03.896647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 14:57:03.897902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1c.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 14:57:03.897934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 1\n",
      "2021-03-25 14:57:04.375010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 14:57:04.375061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      1 \n",
      "2021-03-25 14:57:04.375071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N \n",
      "2021-03-25 14:57:04.375183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 47s 396ms/step - loss: 1.5095 - acc: 0.5883 - val_loss: 1.4435 - val_acc: 0.6438\n",
      "Images/sec: 1345.77\n",
      "Cumulative training time after epoch 1: 46.78\n",
      "Cumulative training time: 46.78\n",
      "Test loss: 1.4475180481910705\n",
      "Test accuracy: 0.6438\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np 1 python fashion_mnist.py --epochs 1 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's modify the above code such that Horovod can automatically do the right thing for any number of training processes. Programmatically, we can arbitrarily select the GPU that corresponds to the Horovod rank and use that one. Since we might be using multiple nodes, and the Horovod rank is a unique identifier across all ranks in the training process, we want to identify our rank locally on the node, which is provided by the `local_rank` specifier:\n",
    "\n",
    "```python\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))\n",
    "```\n",
    "\n",
    "Check out the documentation for both functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "A function that returns the Horovod rank of the calling process.\n",
       "\n",
       "Returns:\n",
       "  An integer scalar with the Horovod rank of the calling process.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.5/dist-packages/horovod/common/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?hvd.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "A function that returns the local Horovod rank of the calling process, within the\n",
       "node that it is running on. For example, if there are seven processes running\n",
       "on a node, their local ranks will be zero through six, inclusive.\n",
       "\n",
       "Returns:\n",
       "  An integer scalar with the local Horovod rank of the calling process.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.5/dist-packages/horovod/common/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?hvd.local_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: with this knowledge in hand, edit `fashion_mnist.py` to pin one GPU to each rank using its local rank ID, immediately after where you have already initialized Horovod.\n",
    "\n",
    "Again, look for `TODO: Step 1` lines in the code. If you get stuck, refer to `solutions/fashion_mnist_after_step_01.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now let's test that you got this right too. Run the training script for just one epoch to make sure everything works. We'll get into the habit of launching with the Horovod job launcher, `horovodrun`, even though for a single process run this is unnecessary. Using `nvidia-smi` in the terminal, verify that only one training process is running, and note which GPU is being used. Is it the one you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:00:19.922772: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:00:23.311089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:00:23.312316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1b.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:00:23.312347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
      "2021-03-25 15:00:23.784995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:00:23.785050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
      "2021-03-25 15:00:23.785060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
      "2021-03-25 15:00:23.785165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 47s 394ms/step - loss: 1.5282 - acc: 0.5984 - val_loss: 0.8956 - val_acc: 0.7039\n",
      "Images/sec: 1347.27\n",
      "Cumulative training time after epoch 1: 46.52\n",
      "Cumulative training time: 46.52\n",
      "Using TensorFlow backend.\n",
      "Test loss: 0.9027660998344421\n",
      "Test accuracy: 0.7024\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np 1 python fashion_mnist.py --epochs 1 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`horovodrun` is a script that launches N copies of the training script, where N is the argument to `-np`. (For those familiar with MPI, it is a thin wrapper around `mpirun`, and in fact it is straightforward to distribute the training using `mpirun` with the right flags.) We'll be using it later to coordinate the training process. Because the processes are launched in the MPI environment, they can communicate between each other through a standardized API that Horovod handles for us, though we haven't instructed the training script to actually coordinate yet; at present we will just launch N independent copies of the same training script. We can try this out now by running as many processes (ranks) as there are GPUs. Watch the output and see if the training process appears to be coordinated -- does the training actually work? Do all of the processes run on the GPU you expect them to, and does that match the output of `nvidia-smi`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:01:51.870754: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:01:51.870862: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:01:51.870969: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:01:51.871056: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:01:55.099145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:01:55.100399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1b.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:01:55.100432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
      "2021-03-25 15:01:55.108030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:01:55.108082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:01:55.113245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1c.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:01:55.113276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 1\n",
      "2021-03-25 15:01:55.113306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:01:55.113337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 3\n",
      "2021-03-25 15:01:55.116887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:01:55.121972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1d.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:01:55.122007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 2\n",
      "2021-03-25 15:01:55.619232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:01:55.619289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
      "2021-03-25 15:01:55.619300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
      "2021-03-25 15:01:55.619413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n",
      "2021-03-25 15:01:55.625912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:01:55.625949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      3 \n",
      "2021-03-25 15:01:55.625958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N \n",
      "2021-03-25 15:01:55.626051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n",
      "2021-03-25 15:01:55.627034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:01:55.627071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      1 \n",
      "2021-03-25 15:01:55.627081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N \n",
      "2021-03-25 15:01:55.627167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n",
      "2021-03-25 15:01:55.651795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:01:55.651839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      2 \n",
      "2021-03-25 15:01:55.651850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N \n",
      "2021-03-25 15:01:55.651953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 48s 407ms/step - loss: 2.2736 - acc: 0.5592 - val_loss: 0.8132 - val_acc: 0.7136\n",
      "Images/sec: 1316.55\n",
      "Cumulative training time after epoch 1: 48.01\n",
      "Cumulative training time: 48.01\n",
      "118/118 [==============================] - 48s 407ms/step - loss: 1.6427 - acc: 0.5894 - val_loss: 0.9608 - val_acc: 0.6437\n",
      "Images/sec: 1318.91\n",
      "Cumulative training time after epoch 1: 47.98\n",
      "Cumulative training time: 47.98\n",
      "118/118 [==============================] - 48s 406ms/step - loss: 3.0675 - acc: 0.5304 - val_loss: 2.8387 - val_acc: 0.5655\n",
      "Images/sec: 1322.2\n",
      "Cumulative training time after epoch 1: 47.94\n",
      "Cumulative training time: 47.94\n",
      "118/118 [==============================] - 48s 409ms/step - loss: 3.3162 - acc: 0.5519 - val_loss: 2.6176 - val_acc: 0.6485\n",
      "Images/sec: 1320.87\n",
      "Cumulative training time after epoch 1: 48.25\n",
      "Cumulative training time: 48.25\n",
      "Using TensorFlow backend.\n",
      "Test loss: 0.8179657713890076\n",
      "Test accuracy: 0.713\n",
      "Test loss: 0.964987752532959\n",
      "Test accuracy: 0.6429\n",
      "Using TensorFlow backend.\n",
      "Test loss: 2.8194782550811768\n",
      "Test accuracy: 0.5668\n",
      "Using TensorFlow backend.\n",
      "Test loss: 2.653810489654541\n",
      "Test accuracy: 0.6447\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np $num_gpus python fashion_mnist.py --epochs 1 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print verbose logs only on the first worker\n",
    "\n",
    "You probably noticed that all N TensorFlow processes printed their progress to stdout (standard output). This results in confusing output -- we only want to see the state of the output once at any given time. To accomplish this, we can arbitrarily select a single rank to display the training progress. By convention, we typically call rank 0 the \"root\" rank and use it for logistical work such as I/O when only one rank is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Edit `fashion_mnist.py` so that you only set `verbose = 1` if it is the first worker (with rank equal to 0) executing the code.\n",
    "\n",
    "Look for `TODO: Step 2` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_02.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the training session to make sure that you now see the expected output. We'll run for 3 epochs this time for comparison to the next exercise. While it's running, you can start working on Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:06:02.470555: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:06:02.470643: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:06:02.470851: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:06:02.471044: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:06:05.601303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:06:05.602371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:06:05.603057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:06:05.608569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:06:05.608608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 3\n",
      "2021-03-25 15:06:05.609865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1b.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:06:05.609897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
      "2021-03-25 15:06:05.611226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1c.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:06:05.611257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 1\n",
      "2021-03-25 15:06:05.611556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:06:05.619319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1d.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:06:05.619357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 2\n",
      "2021-03-25 15:06:06.225659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:06:06.225713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
      "2021-03-25 15:06:06.225724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
      "2021-03-25 15:06:06.225845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n",
      "2021-03-25 15:06:06.270284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:06:06.270337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      2 \n",
      "2021-03-25 15:06:06.270354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N \n",
      "2021-03-25 15:06:06.270494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n",
      "2021-03-25 15:06:06.278203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:06:06.278241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      3 \n",
      "2021-03-25 15:06:06.278250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N \n",
      "2021-03-25 15:06:06.278341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n",
      "2021-03-25 15:06:06.287047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:06:06.287082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      1 \n",
      "2021-03-25 15:06:06.287091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N \n",
      "2021-03-25 15:06:06.287183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n",
      "Epoch 1/3\n",
      "118/118 [==============================] - 48s 404ms/step - loss: 2.1399 - acc: 0.5633 - val_loss: 1.0680 - val_acc: 0.6282\n",
      "Images/sec: 1325.03\n",
      "Cumulative training time after epoch 1: 47.7\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 37s 317ms/step - loss: 0.7124 - acc: 0.7507 - val_loss: 0.8219 - val_acc: 0.6966\n",
      "Images/sec: 1605.64\n",
      "Cumulative training time after epoch 2: 85.07\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 37s 317ms/step - loss: 0.6253 - acc: 0.7788 - val_loss: 0.6942 - val_acc: 0.7600\n",
      "Images/sec: 1606.41\n",
      "Cumulative training time after epoch 3: 122.43\n",
      "Cumulative training time: 122.43\n",
      "Test loss: 0.6948977486610413\n",
      "Test accuracy: 0.7594\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np $num_gpus python fashion_mnist.py --epochs 3 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add distributed optimizer\n",
    "\n",
    "Horovod uses an operation that averages gradients across workers. Implementing this is very straightforward and just requires wrapping an existing optimizer (`keras.optimizers.Optimizer`) with a Horovod distributed optimizer (`horovod.keras.DistributedOptimizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice_dense\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'horovod.tensorflow.compression.NoneCompressor'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparse_as_dense\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "An optimizer that wraps another keras.optimizers.Optimizer, using an allreduce to\n",
       "average gradient values before applying gradients to model weights.\n",
       "\n",
       "Args:\n",
       "    optimizer: Optimizer to use for computing gradients and applying updates.\n",
       "    name: Optional name prefix for the operations created when applying\n",
       "          gradients. Defaults to \"Distributed\" followed by the provided\n",
       "          optimizer type.\n",
       "    device_dense: Device to be used for dense tensors. Uses GPU by default\n",
       "                  if Horovod was build with HOROVOD_GPU_ALLREDUCE.\n",
       "    device_sparse: Device to be used for sparse tensors. Uses GPU by default\n",
       "                   if Horovod was build with HOROVOD_GPU_ALLGATHER.\n",
       "    compression: Compression algorithm used to reduce the amount of data\n",
       "                 sent and received by each worker node.  Defaults to not\n",
       "                 using compression.\n",
       "    sparse_as_dense: Treat all sparse gradients as dense tensors.  This can\n",
       "                     help improve performance and memory utilization if\n",
       "                     the original sparse gradient has high density.\n",
       "                     Defaults to false.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.5/dist-packages/horovod/keras/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?hvd.DistributedOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: wrap the optimizer (`opt` in `fashion_mnist.py`) with a Horovod distributed optimizer.\n",
    "\n",
    "Look for `TODO: Step 3` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_03.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the training now and see if you get a reasonable answer. Is the accuracy any better? Note that we are only training for a few epochs, and the results will depend on the initial random weights, so do not draw any strong conclusions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!horovodrun -np $num_gpus python fashion_mnist.py --epochs 3 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize random weights on only one processor\n",
    "\n",
    "Data parallel stochastic gradient descent, at least in its traditionally defined sequential algorithm, requires weights to be synchronized between all processors. We already know that this is accomplished for backpropagation by averaging out the gradients among all processors prior to the weight updates. Then the only other required step is for the weights to be synchronized initially. Assuming we start from the beginning of the training (we won't handle checkpoint/restart in this lab, but [it is a straightforward extension](https://horovod.readthedocs.io/en/latest/keras.html)), this means that every processor needs to have the same random weights.\n",
    "\n",
    "\n",
    "In a previous section, we mentioned that the first worker would broadcast parameters to the rest of the workers.  We will use `horovod.keras.callbacks.BroadcastGlobalVariablesCallback` to make this happen. Execute the following cell to get more information about the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.callbacks.BroadcastGlobalVariablesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: append this callback to our list of callbacks. Note the argument required for this callback, the rank of the root worker.\n",
    "\n",
    "Look for `TODO: Step 4` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_04.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, run the training session for one epoch just to make sure things work, and notice if it affected the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!horovodrun -np $num_gpus python fashion_mnist.py --epochs 3 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modify training loop to execute fewer steps per epoch\n",
    "\n",
    "As it stands, we are running the same number of steps per epoch for the serial training implementation. But since we have increased the number of workers by a factor of N, that means we're doing N times more work (when we sum the amount of work done over all processes). Our target was to get the *same* answer in less time (that is, to speed up the training), so we want to keep the total amount of work done the same (that is, to process the same number of examples in the dataset). This means we need to do a factor of N *fewer* steps per epoch, so the number of steps goes to `steps_per_epoch / number_of_workers`.\n",
    "\n",
    "We will also speed up validation by validating `3 * num_test_iterations / number_of_workers` steps on each worker. While we could just do `num_test_iterations / number_of_workers` on each worker to get a linear speedup in the validation, the multiplier **3** provides over-sampling of the validation data and helps to increase the probability that every validation example will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: modify the `steps_per_epoch` and `validation_steps` arguments for `model.fit_generator` to follow the plan just outlined. This environment uses Python 3, and each of these arguments expect integers, so take care to round any potential floating point values down to the nearest integer.\n",
    "\n",
    "Look for `TODO: Step 5` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_05.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!horovodrun -np $num_gpus python fashion_mnist.py --epochs 3 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Average validation results among workers\n",
    "\n",
    "Since we are not validating the full dataset on each worker anymore, each worker will have different validation results. To improve validation metric quality and reduce variance, we will average validation results among all workers.\n",
    "\n",
    "To do so, we can use `horovod.keras.callbacks.MetricAverageCallback`. Execute the following cell to get more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetricAverageCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Keras Callback that will average metrics across all processes at the\n",
       "end of the epoch. Useful in conjuction with ReduceLROnPlateau,\n",
       "TensorBoard and other metrics-based callbacks.\n",
       "\n",
       "Note: This callback must be added to the callback list before the\n",
       "ReduceLROnPlateau, TensorBoard or other metrics-based callbacks.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Construct a new MetricAverageCallback that will average metrics\n",
       "across all processes at the end of the epoch.\n",
       "\n",
       "Args:\n",
       "    device: Device to be used for allreduce. Uses GPU by default\n",
       "            if Horovod was build with HOROVOD_GPU_ALLREDUCE.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.5/dist-packages/horovod/keras/callbacks.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?hvd.callbacks.MetricAverageCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: average the metrics among workers at the end of every epoch by injecting `MetricAverageCallback` after `BroadcastGlobalVariablesCallback`. Please note that this callback must be in the list before other metrics-based callbacks, `ReduceLROnPlateau`, `TensorBoard`, etc.\n",
    "\n",
    "Look for `TODO: Step 6` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_06.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 15:24:33.108270: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:24:33.108598: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:24:33.108668: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:24:33.108913: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-25 15:24:36.426201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:24:36.426221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:24:36.429265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:24:36.429362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-03-25 15:24:36.430177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:24:36.430214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 3\n",
      "2021-03-25 15:24:36.430233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1c.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:24:36.430260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 1\n",
      "2021-03-25 15:24:36.434841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1d.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:24:36.434873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 2\n",
      "2021-03-25 15:24:36.434959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:1b.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.47GiB\n",
      "2021-03-25 15:24:36.434990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
      "2021-03-25 15:24:36.948089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:24:36.948145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      2 \n",
      "2021-03-25 15:24:36.948155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N \n",
      "2021-03-25 15:24:36.948264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n",
      "2021-03-25 15:24:36.948289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:24:36.948314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
      "2021-03-25 15:24:36.948322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
      "2021-03-25 15:24:36.948412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n",
      "2021-03-25 15:24:36.954749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:24:36.954788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      1 \n",
      "2021-03-25 15:24:36.954797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N \n",
      "2021-03-25 15:24:36.954889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n",
      "2021-03-25 15:24:36.960488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-25 15:24:36.960527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      3 \n",
      "2021-03-25 15:24:36.960537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N \n",
      "2021-03-25 15:24:36.960627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14954 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Traceback (most recent call last):\n",
      "  File \"fashion_mnist.py\", line 210, in <module>\n",
      "    steps_per_epoch=len(train_iter) // hv.size(),\n",
      "NameError: name 'hv' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"fashion_mnist.py\", line 210, in <module>\n",
      "    steps_per_epoch=len(train_iter) // hv.size(),\n",
      "NameError: name 'hv' is not defined\n",
      "Using TensorFlow backend.\n",
      "Traceback (most recent call last):\n",
      "  File \"fashion_mnist.py\", line 210, in <module>\n",
      "    steps_per_epoch=len(train_iter) // hv.size(),\n",
      "NameError: name 'hv' is not defined\n",
      "Using TensorFlow backend.\n",
      "Traceback (most recent call last):\n",
      "  File \"fashion_mnist.py\", line 210, in <module>\n",
      "    steps_per_epoch=len(train_iter) // hv.size(),\n",
      "NameError: name 'hv' is not defined\n",
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fb10954ae48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 738, in __del__\n",
      "TypeError: 'NoneType' object is not callable\n",
      "-------------------------------------------------------\n",
      "Primary job  terminated normally, but 1 process returned\n",
      "a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "-------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "mpirun.real detected that one or more processes exited with non-zero status, thus causing\n",
      "the job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[43872,1],2]\n",
      "  Exit code:    1\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np $num_gpus python fashion_mnist.py --epochs 3 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. CPU Comparison\n",
    "\n",
    "Now that you've implemented the training on multiple GPUs, let's look at a comparison, training on just the CPU. You can do this by setting `CUDA_VISIBLE_DEVICES` to an empty string. You should see quite a difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES= python fashion_mnist.py --epochs 1 --batch-size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your work\n",
    "\n",
    "Congratulations!  If you made it this far, your `fashion_mnist.py` should now be fully distributed. To verify, compare `fashion_mnist.py` to `fashion_mnist_solution.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
